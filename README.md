<div align="center">
  <H1>
    Выбор интерпретируемых рекуррентных моделей глубокого обучения
  </H1>
  Гапонов Максим Евгеньевич
</div><br>
<div align="center">
  Научные руководители: Бахтеев Олег, Яковлев Константин, Стрижов Вадим <br>
</div>

## Аннотация
Рассматривается задача интерпретации рекуррентных нейронных сетей. Под интерпретируемостью модели понимается возможность получить линейную зависимость выходных данных от входных. Предлагается обобщить метод OpenBox, предназначенный для интерпретации нейронных сетей с кусочно-линейными функциями активации. В данном методе нейронная сеть представляется в виде ансамбля интерпретируемых линейных классификаторов, каждый из которых определён на выпуклом многограннике. Поэтому интерпретации близких объектов согласованы. Для проверки работоспособности предложенного метода проводится эксперимент на выборке IMDB Review dataset.


## Abstract
The problem of interpretation of recurrent neural networks is considered. The interpretability of the model is understood as the ability to obtain a linear dependence of the output data on the input data. It is proposed to generalize the OpenBox method designed for the interpretation of neural networks with piecewise linear activation functions. In this method, the neural network is represented as an ensemble of interpreted linear classifiers, each of which is defined on a convex polyhedron. Therefore, the interpretations of close objects are consistent. To test the operability of the proposed method, an experiment is conducted on a sample of the IMDB Review dataset.
