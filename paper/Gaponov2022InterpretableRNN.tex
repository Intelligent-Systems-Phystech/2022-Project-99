\documentclass[12pt]{article}
\usepackage{arxiv}
\usepackage[utf8]{inputenc}
\usepackage[english, russian]{babel}
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{mathtools}

\renewcommand{\shorttitle}{\textit{arXiv} Template}

\begin{document}
\title
{Выбор интерпретируемых рекуррентных моделей глубокого обучения}
\author{Гапонов Максим \\
	\texttt{gaponov.me@phystech.edu} \\
	\And
	Бахтеев Олег \\
	\texttt{bakhteev@phystech.edu} \\
	\And
	Яковлев Константин \\
	\texttt{...} \\
	\And
	Стрижов Вадим \\
	\texttt{...} \\
}
\date{}
\maketitle
\begin{abstract}
В данной работе рассматривается задача интерпретации рекуррентных нейронных сетей. Интерпретируемость нейронных сетей необходима для повышения уровня доверия к предсказаниям таких моделей. Для решения задачи предлагается обобщить метод OpenBox, предназначенный для кусочно-линейных нейронных сетей. Модель представляется в виде набора интерпретируемых линейных классификаторов. Каждый из классификаторов определён на выпуклом многограннике, поэтому интерпретации близких объектов оказываются согласованными. Для проверки работоспособности предложенного метода проводится эксперимент на выборке DBLP (данные о цитированиях).
\end{abstract}

% \linenumbers
\section{Введение}
В работе рассматривается метод интерпретации рекуррентных моделей глубокого обучения. Такие модели используются для обработки последовательностей данных \cite{lipton2015critical}. Однако для принятия решения при помощи такой модели необходимо доверять её предсказаниям. По этой причине необходимо построить интерпретируемую модель, объясняющую предсказания исходной модели. Работа посвящена построению и выбору таких интерпретируемых моделей для произвольной рекуррентной сети.

Рекуррентная нейронная сеть представляет из себя последовательность из слоёв нейронов: входного слоя, скрытых слоёв и выходного слоя \cite{sherstinsky2020fundrnn}. Отличительной особенностью от обыкновенных нейронных сетей являются циклические связи между нейронами. Такие связи позволяют запоминать состояния для обработки последовательностей данных.

Интерпретация должна быть \textbf{точной} и \textbf{согласованной} \cite{chu2019exact}. Интерпретация называется точной, если она согласуется с поведением модели. То есть показывает значимыми те признаки, которые значительно повлияли на предсказание модели. Интерпретация называется согласованной, если интерпретации близких объектов похожи на исходную интерпретацию. То есть метод похожим образом объясняет предсказания на близких объектах.

Анализ зависимости предсказания модели от входных данных в реккурентных нейронных сетях является сложной задачей. Методы, рассмотренные в работах \cite{dosovitskiy2016inverting} \cite{zhou2018interpreting} \cite{NIPS2014_ea8fcd92} \cite{bastani2019interpreting} \cite{zhou2015learning} \cite{simonyan2014deep} являются либо неточными, то есть не отвечают реальному поведению модели, либо несогласованными, то есть совершенно разным образом объясняет предсказания на близких объектах.

В работах \cite{dosovitskiy2016inverting} \cite{zhou2018interpreting} рассмотрены методы, основанные на интерпретации признаков, выученных скрытыми слоями. Такой подход отражает поведение скрытых слоёв, но не показывает поведение сети в целом.

В работах \cite{NIPS2014_ea8fcd92} \cite{bastani2019interpreting} предлагаются методы подражания модели. В этом подходе строится модель с похожим на исходную поведением. Построенная модель имеет более простую структуру, поэтому проще интерпретируется. Однако из-за недостаточно сложной структуры построенная модель недостаточно хорошо приближает исходную.

Наконец, в работах \cite{zhou2015learning} \cite{simonyan2014deep} рассмотрены методы локальной интерпретации, в которых происходит анализ поведения модели в окрестности исходного объекта. В таком подходе получается точная интерпретацию для одного объекта, однако интерпретации для близких объектов могут существенно отличаться. Таким образом, данный подход не предоставляет согласованные интерпретации.

Рассмотренный в данной работе подход является обобщением метода OpenBox. В \cite{chu2019exact} предложен метод OpenBox для интерпретации кусочно-линейных нейронных сетей. Модель представляется в виде эквивалентного набора линейных классификаторов. Каждый из них классифицирует объекты внутри некоторого выпуклого многогранника в пространстве признаков, поэтому интерпретации получаются согласованными.

На выборке DBLP проведён вычислительный эксперимент для анализа качества метода, а также для проверки интерпретаций на точность и согласованность. 

\section{Постановка задачи}
Рассмотрим рекуррентную нейронную сеть $\mathcal{N}$ с кусочно-линейными функциями активации. Входные данные обозначим через $x \in \mathcal{X}$, где $\mathcal{X} \subset \mathbb{R}^d~-$ пространство признаков. Предсказание модели $\mathcal{N}$ обозначим через $y \in \mathcal{Y}$, где $\mathcal{Y}~-$ пространство предсказаний.

Модель $\mathcal{N}$ представляет из себя функцию классификации $F: \mathcal{X} \to \mathcal{Y}$. Из-за сложной структуры сети трудно понять поведение функции $F$, поэтому необходимо получить интерпретацию, которую способен понять человек.

Функция $F$ является кусочно-линейной, так как $\mathcal{N}~-$ рекуррентная нейронная сеть с кусочно-линейными функциями активации. Области, на которых функция $F$ линейна являются выпуклыми многогранниками.

Предлагается рассмотреть модель $\mathcal{M}$, представляющую из себя множество линейных классификаторов $\left\{F_1, F_2, \dots, F_h, \dots\right\}$. Пространство признаков $\mathcal{X}$ разбивается на конечное множество выпуклых многогранников $\left\{P_1, P_2, \dots, P_h, \dots\right\}$. То есть $\bigsqcup\limits_{h} P_h=\mathcal{X}$. Классификатор $F_h$ определён на многограннике $P_h$. Модель $\mathcal{M}$ должна быть математически эквивалентна модели $\mathcal{N}$, то есть множество функций $\left\{F_1, F_2, \dots, F_h, \dots\right\}$ определённых на $\left\{P_1, P_2, \dots, P_h, \dots\right\}$ тождественно равно функции $F$, определённой на $\mathcal{X}$.

Модель $\mathcal{M}$ обладает свойством точности, так как математически эквивалентна модели $\mathcal{N}$. То есть поведение двух моделей совпадает. $F|_{P_h} \equiv F_h$. Модель $\mathcal{M}$ также обладает свойством согласованности, так как близкие объекты попадают в один и тот же выпуклый многогранник $P_h$, а значит, классифицируются одним и тем же линейным классификатором $F_h$.

Задача состоит в поиске такой модели $\mathcal{M}$, построении множества классификаторов по произвольной рекуррентной нейронной сети $\mathcal{N}$.

\bibliographystyle{plain}
\bibliography{Gaponov2022InterpretableRNN}

\end{document}
